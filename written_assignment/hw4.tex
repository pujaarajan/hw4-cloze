\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Machine Translation HW4 Multi-word Cloze}
\author{Tanay Agarwal (tagarwa2), Tomas Ferrer (tferrer1), and Jonathan Liu (jliu118)}
\date{November 5, 2017}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[margin=1in]{geometry}

\begin{document}

\maketitle

Code can be found in the following \href{https://github.com/tferrer1/mt-clauze}{Github repository}

\section{Part 1}

We implemented all of Shuoyang's hints. Our dev negative log probability after epoch 0 was 5.44, which passes our sanity check. Our converged dev negative log probability was 5.10 after epoch 3.

Usage instructions:
To obtain the models, run the following command:
\begin{verbatim}
python train.py --data_file hw4_data --optimizer Adam -lr 1e-2 --batch_size 48 --model_file model.py
\end{verbatim}

\section{Part 2}

We implemented all of Shuoyang's hints. Our dev negative log probability after epoch 0 was 4.75, which passes our sanity check. Our converged dev negative log probability was 4.19 after epoch 6.

Usage instructions:
To obtain the models, run the following command:
\begin{verbatim}
python train_bi.py --data_file hw4_data --optimizer Adam -lr 1e-2 --batch_size 48 --model_file model.py
\end{verbatim}

\section{Part 3}

We improved BiRNN by implementing dropout. We won't explain dropout in too much theoretical detail because we provide links that have great walkthroughs (and the concept is not very complicated). Below is an overview that can be used to easily reproduce our scores with dropout:

The main concept of dropout is to prevent overfitting by turning nodes off randomly during forward-propogation (but not during backpropogation). We basically want to save computational and memory resources, and turning nodes off randomly does this by helping prevent nodes from converging to the same local minima. This makes the computations less redundant.

Dropout is used during training only, and not during testing/runtime. In order to implement dropout, we simply have to randomly turn off nodes. We do this by going layer-by-layer and applying a binomial mask to the layer. The binomial mask basically "selects" which nodes we want to use and which nodes we want to turn off in this particular forward pass. After turning off some nodes, we simply scale the layer according to how many nodes were turned off (in order to maintain the amount of information flowing through the layer). Some in-depth code can be found here: http://iamtrask.github.io/2015/07/28/dropout/

We experimented a little with the hyperparameters and found that we got the best results using an embedding size of 50, hidden size of 25, and dropout rate of 0.2. Our converged dev negative log probability was 3.97 after epoch 9. Our final cloze score on the leaderboard using this model was 0.298. 

Usage instructions:
To obtain the BiRNN with Dropout model, run the following command:
\begin{verbatim}
python train_dropout.py --data_file hw4_data --optimizer Adam -lr 1e-2 --batch_size 48 --model_file model.py
\end{verbatim}
To predict the clozes, modify the script to include the name of the model you want to use, and run the following command:
\begin{verbatim}
python predict_clozes.py
\end{verbatim}

\section{Part 4}

Running on GPU provided a very significant speedup compared to CPU. We measured runtimes on our improvement model (BiRNN with dropout). We used the default given embedding and batch sizes to begin with. This gave us a runtime of 10 minutes and 34 seconds on CPU, and only 32 seconds on GPU!

Using a batch size of 100 sped up computation even more: we got a speed of 6 minutes and 8 seconds on CPU, and only 23 seconds on GPU!

It is clear to us that GPUs are extremely powerful, and that the speedup compared to CPUs is very significant.

Usage instructions:
On a system that has a GPU, run the following command:
\begin{verbatim}
python train_cuda_dropout.py --data_file hw4_data --optimizer Adam -lr 1e-2 --batch_size 48 --model_file cuda_model.py
\end{verbatim}
To predict the clozes, simply modify the script to include the name of the model you want to use, and run the following command:
\begin{verbatim}
python predict_clozes.py
\end{verbatim}
